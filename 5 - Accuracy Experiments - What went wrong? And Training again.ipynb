{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10fcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory, image \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, InputLayer,Dropout\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff85baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d278686",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd()/'data'\n",
    "answers = pd.read_csv('answers.csv')\n",
    "images_directory = path/'train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808abfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 files belonging to 2 classes.\n",
      "Using 24000 files for training.\n",
      "Found 30000 files belonging to 2 classes.\n",
      "Using 6000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "cb_training = image_dataset_from_directory(images_directory, labels='inferred', image_size=(97,97), subset='training', validation_split=.2, seed=10)\n",
    "cb_validation = image_dataset_from_directory(images_directory, labels='inferred', image_size=(97,97), subset='validation', validation_split=.2, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880a288",
   "metadata": {},
   "source": [
    "# Review \n",
    "## Terrible Accuracy \n",
    "So the accuracy is terrible. Why? \n",
    "* I adjusted the input \n",
    "* I adjusted the class weights \n",
    "* Transfer Learning - maybe this didnt actually help? \n",
    "\n",
    "## Checking What Happened \n",
    "We need to figure out why the accuracy dropped so much. So lets go back to the original CNN model with the new spectrograms, and rerun it. \n",
    "### Experiment 1: Run model w/ original class weights, but new spectrograms \n",
    "In notebook 3, we were getting .9 accuracy, even if we were overfitting grossly. If its the new spectrograms, then running that model w/ other parameters the same should drop the accuracy like a rock. \n",
    "\n",
    "### Experiment 2: The class Weights \n",
    "If the new spectrograms dont make the difference, then we also adjusted the class weights. Run the original model with the new class weights, and see if the accuracy drops.  \n",
    "\n",
    "### Experiment 3: Dense Layers w/ original CNN output\n",
    "Compare dense layers from the transfer learning from notebook 4 w/ dense layers from notebook 3, using the original sigmoid output layer. Perhaps they are too large. Additionally, the activation function is softmax instead of sigmoid. Using VGG19 + dense layers from my original CNN and original output layer, can we compare it to the VGG19+dense layers from notebook 4 \n",
    "\n",
    "### Experiment 4: The output layer\n",
    "The tropical paper uses softmax, whereas my model uses sigmoid. Since we only have 2 classes, maybe sigmoid would work better? Using the better(by loss) model from experiment 3, try both. \n",
    "\n",
    "# Below Is the Original CNN Model\n",
    "The input size is (64, 64, 3)  \n",
    "We have changed to (97, 97, 3) so I will make that adjustment. Everything else is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f146a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 97, 97, 5)         140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 10)        460       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 20)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 30)        5430      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 30)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 40)          10840     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 31,811\n",
      "Trainable params: 31,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(InputLayer(input_shape=(97,97, 3)))\n",
    "cnn.add(Conv2D(filters=5, kernel_size=3, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling2D())\n",
    "cnn.add(Conv2D(filters=10, kernel_size=3, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling2D())\n",
    "cnn.add(Conv2D(filters=20, kernel_size=3, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling2D())\n",
    "cnn.add(Conv2D(filters=30, kernel_size=3, activation='relu', padding='same'))\n",
    "cnn.add(MaxPooling2D())\n",
    "cnn.add(Conv2D(filters=40, kernel_size=3, activation='relu', padding='same'))\n",
    "cnn.add(GlobalAveragePooling2D())\n",
    "\n",
    "cnn.add(layers.Dense(20, activation='relu'))\n",
    "cnn.add(layers.Dense(100, activation='relu'))\n",
    "cnn.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0ec72",
   "metadata": {},
   "source": [
    "# Experiment 1 - Was it the Spectrograms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328e9417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 80s 101ms/step - loss: 0.2390 - tp: 4634.0000 - fp: 3179.0000 - tn: 15193.0000 - fn: 994.0000 - accuracy: 0.8261 - precision: 0.5931 - recall: 0.8234 - auc: 0.8971 - prc: 0.6890 - val_loss: 0.2723 - val_tp: 1280.0000 - val_fp: 699.0000 - val_tn: 3902.0000 - val_fn: 119.0000 - val_accuracy: 0.8637 - val_precision: 0.6468 - val_recall: 0.9149 - val_auc: 0.9469 - val_prc: 0.8135\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 76s 101ms/step - loss: 0.1617 - tp: 5041.0000 - fp: 2194.0000 - tn: 16178.0000 - fn: 587.0000 - accuracy: 0.8841 - precision: 0.6968 - recall: 0.8957 - auc: 0.9517 - prc: 0.8269 - val_loss: 0.2357 - val_tp: 1190.0000 - val_fp: 436.0000 - val_tn: 4165.0000 - val_fn: 209.0000 - val_accuracy: 0.8925 - val_precision: 0.7319 - val_recall: 0.8506 - val_auc: 0.9545 - val_prc: 0.8387\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 76s 102ms/step - loss: 0.1478 - tp: 5104.0000 - fp: 1991.0000 - tn: 16381.0000 - fn: 524.0000 - accuracy: 0.8952 - precision: 0.7194 - recall: 0.9069 - auc: 0.9583 - prc: 0.8509 - val_loss: 0.2269 - val_tp: 1263.0000 - val_fp: 498.0000 - val_tn: 4103.0000 - val_fn: 136.0000 - val_accuracy: 0.8943 - val_precision: 0.7172 - val_recall: 0.9028 - val_auc: 0.9616 - val_prc: 0.8659\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 74s 99ms/step - loss: 0.1388 - tp: 5128.0000 - fp: 1844.0000 - tn: 16528.0000 - fn: 500.0000 - accuracy: 0.9023 - precision: 0.7355 - recall: 0.9112 - auc: 0.9635 - prc: 0.8722 - val_loss: 0.2135 - val_tp: 1245.0000 - val_fp: 422.0000 - val_tn: 4179.0000 - val_fn: 154.0000 - val_accuracy: 0.9040 - val_precision: 0.7469 - val_recall: 0.8899 - val_auc: 0.9639 - val_prc: 0.8783\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 77s 102ms/step - loss: 0.1313 - tp: 5141.0000 - fp: 1743.0000 - tn: 16629.0000 - fn: 487.0000 - accuracy: 0.9071 - precision: 0.7468 - recall: 0.9135 - auc: 0.9679 - prc: 0.8897 - val_loss: 0.2284 - val_tp: 1278.0000 - val_fp: 507.0000 - val_tn: 4094.0000 - val_fn: 121.0000 - val_accuracy: 0.8953 - val_precision: 0.7160 - val_recall: 0.9135 - val_auc: 0.9659 - val_prc: 0.8835\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 75s 101ms/step - loss: 0.1229 - tp: 5165.0000 - fp: 1565.0000 - tn: 16807.0000 - fn: 463.0000 - accuracy: 0.9155 - precision: 0.7675 - recall: 0.9177 - auc: 0.9720 - prc: 0.9045 - val_loss: 0.1950 - val_tp: 1169.0000 - val_fp: 282.0000 - val_tn: 4319.0000 - val_fn: 230.0000 - val_accuracy: 0.9147 - val_precision: 0.8057 - val_recall: 0.8356 - val_auc: 0.9669 - val_prc: 0.8937\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 76s 101ms/step - loss: 0.1221 - tp: 5168.0000 - fp: 1589.0000 - tn: 16783.0000 - fn: 460.0000 - accuracy: 0.9146 - precision: 0.7648 - recall: 0.9183 - auc: 0.9725 - prc: 0.9065 - val_loss: 0.2019 - val_tp: 1237.0000 - val_fp: 376.0000 - val_tn: 4225.0000 - val_fn: 162.0000 - val_accuracy: 0.9103 - val_precision: 0.7669 - val_recall: 0.8842 - val_auc: 0.9692 - val_prc: 0.8966\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 76s 102ms/step - loss: 0.1144 - tp: 5180.0000 - fp: 1420.0000 - tn: 16952.0000 - fn: 448.0000 - accuracy: 0.9222 - precision: 0.7848 - recall: 0.9204 - auc: 0.9761 - prc: 0.9199 - val_loss: 0.2060 - val_tp: 1248.0000 - val_fp: 422.0000 - val_tn: 4179.0000 - val_fn: 151.0000 - val_accuracy: 0.9045 - val_precision: 0.7473 - val_recall: 0.8921 - val_auc: 0.9657 - val_prc: 0.8905\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 77s 102ms/step - loss: 0.1101 - tp: 5198.0000 - fp: 1392.0000 - tn: 16980.0000 - fn: 430.0000 - accuracy: 0.9241 - precision: 0.7888 - recall: 0.9236 - auc: 0.9777 - prc: 0.9248 - val_loss: 0.2367 - val_tp: 1311.0000 - val_fp: 542.0000 - val_tn: 4059.0000 - val_fn: 88.0000 - val_accuracy: 0.8950 - val_precision: 0.7075 - val_recall: 0.9371 - val_auc: 0.9669 - val_prc: 0.8881\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 72s 97ms/step - loss: 0.1106 - tp: 5226.0000 - fp: 1418.0000 - tn: 16954.0000 - fn: 402.0000 - accuracy: 0.9242 - precision: 0.7866 - recall: 0.9286 - auc: 0.9774 - prc: 0.9236 - val_loss: 0.2143 - val_tp: 1292.0000 - val_fp: 474.0000 - val_tn: 4127.0000 - val_fn: 107.0000 - val_accuracy: 0.9032 - val_precision: 0.7316 - val_recall: 0.9235 - val_auc: 0.9704 - val_prc: 0.8990\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "class_weight = {0: .5, 1:1}\n",
    "history = cnn.fit(cb_training, epochs=10,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5354e",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "Below is the historical output from running this model in Notebook 3 on the slightly different histrograms. \n",
    "Already we see that the output is comprable - perhaps slightly lower on the starting accuracy, but in the correct vicinity for sure. Accuracy is sitting at .9, historical was .92. Precision and recall are pretty high too, with precision being a little bit lower - this matches the historical pattern as well. We also see that the validation is performing worse because we're overfitting - this is expected, and matches historicals well.  \n",
    "While the spectrograms might be responsible for a small percentage of accuracy drop, we're not seeing the severe drop we saw in the transfer learning. \n",
    "\n",
    "### Conclusion: Likely not the Adjusted Spectrogram and Size\n",
    "```\n",
    "Epoch 1/50\n",
    "750/750 [==============================] - 38s 47ms/step - loss: 0.1075 - tp: 5201.0000 - fp: 1342.0000 - tn: 17030.0000 - fn: 427.0000 - accuracy: 0.9263 - precision: 0.7949 - recall: 0.9241 - auc: 0.9790 - prc: 0.9294 - val_loss: 0.2400 - val_tp: 1282.0000 - val_fp: 505.0000 - val_tn: 4096.0000 - val_fn: 117.0000 - val_accuracy: 0.8963 - val_precision: 0.7174 - val_recall: 0.9164 - val_auc: 0.9654 - val_prc: 0.8937\n",
    "Epoch 2/50\n",
    "750/750 [==============================] - 34s 46ms/step - loss: 0.1047 - tp: 5217.0000 - fp: 1287.0000 - tn: 17085.0000 - fn: 411.0000 - accuracy: 0.9293 - precision: 0.8021 - recall: 0.9270 - auc: 0.9802 - prc: 0.9331 - val_loss: 0.2090 - val_tp: 1211.0000 - val_fp: 368.0000 - val_tn: 4233.0000 - val_fn: 188.0000 - val_accuracy: 0.9073 - val_precision: 0.7669 - val_recall: 0.8656 - val_auc: 0.9651 - val_prc: 0.8896\n",
    "Epoch 3/50\n",
    "750/750 [==============================] - 32s 43ms/step - loss: 0.1028 - tp: 5250.0000 - fp: 1288.0000 - tn: 17084.0000 - fn: 378.0000 - accuracy: 0.9306 - precision: 0.8030 - recall: 0.9328 - auc: 0.9804 - prc: 0.9331 - val_loss: 0.2577 - val_tp: 1307.0000 - val_fp: 606.0000 - val_tn: 3995.0000 - val_fn: 92.0000 - val_accuracy: 0.8837 - val_precision: 0.6832 - val_recall: 0.9342 - val_auc: 0.9640 - val_prc: 0.8871\n",
    "Epoch 4/50\n",
    "750/750 [==============================] - 30s 40ms/step - loss: 0.0993 - tp: 5257.0000 - fp: 1227.0000 - tn: 17145.0000 - fn: 371.0000 - accuracy: 0.9334 - precision: 0.8108 - recall: 0.9341 - auc: 0.9819 - prc: 0.9380 - val_loss: 0.2320 - val_tp: 1264.0000 - val_fp: 467.0000 - val_tn: 4134.0000 - val_fn: 135.0000 - val_accuracy: 0.8997 - val_precision: 0.7302 - val_recall: 0.9035 - val_auc: 0.9648 - val_prc: 0.8953\n",
    "Epoch 5/50\n",
    "750/750 [==============================] - 36s 47ms/step - loss: 0.0970 - tp: 5276.0000 - fp: 1235.0000 - tn: 17137.0000 - fn: 352.0000 - accuracy: 0.9339 - precision: 0.8103 - recall: 0.9375 - auc: 0.9827 - prc: 0.9403 - val_loss: 0.2161 - val_tp: 1232.0000 - val_fp: 406.0000 - val_tn: 4195.0000 - val_fn: 167.0000 - val_accuracy: 0.9045 - val_precision: 0.7521 - val_recall: 0.8806 - val_auc: 0.9638 - val_prc: 0.8876\n",
    "Epoch 6/50\n",
    "750/750 [==============================] - 30s 40ms/step - loss: 0.0952 - tp: 5309.0000 - fp: 1263.0000 - tn: 17109.0000 - fn: 319.0000 - accuracy: 0.9341 - precision: 0.8078 - recall: 0.9433 - auc: 0.9831 - prc: 0.9399 - val_loss: 0.2231 - val_tp: 1269.0000 - val_fp: 463.0000 - val_tn: 4138.0000 - val_fn: 130.0000 - val_accuracy: 0.9012 - val_precision: 0.7327 - val_recall: 0.9071 - val_auc: 0.9652 - val_prc: 0.8932\n",
    "Epoch 7/50\n",
    "750/750 [==============================] - 31s 42ms/step - loss: 0.0922 - tp: 5327.0000 - fp: 1235.0000 - tn: 17137.0000 - fn: 301.0000 - accuracy: 0.9360 - precision: 0.8118 - recall: 0.9465 - auc: 0.9840 - prc: 0.9448 - val_loss: 0.2125 - val_tp: 1241.0000 - val_fp: 387.0000 - val_tn: 4214.0000 - val_fn: 158.0000 - val_accuracy: 0.9092 - val_precision: 0.7623 - val_recall: 0.8871 - val_auc: 0.9643 - val_prc: 0.8929\n",
    "Epoch 8/50\n",
    "750/750 [==============================] - 36s 48ms/step - loss: 0.0911 - tp: 5319.0000 - fp: 1178.0000 - tn: 17194.0000 - fn: 309.0000 - accuracy: 0.9380 - precision: 0.8187 - recall: 0.9451 - auc: 0.9845 - prc: 0.9453 - val_loss: 0.2419 - val_tp: 1274.0000 - val_fp: 486.0000 - val_tn: 4115.0000 - val_fn: 125.0000 - val_accuracy: 0.8982 - val_precision: 0.7239 - val_recall: 0.9107 - val_auc: 0.9648 - val_prc: 0.8852\n",
    "Epoch 9/50\n",
    "750/750 [==============================] - 32s 43ms/step - loss: 0.0866 - tp: 5317.0000 - fp: 1098.0000 - tn: 17274.0000 - fn: 311.0000 - accuracy: 0.9413 - precision: 0.8288 - recall: 0.9447 - auc: 0.9859 - prc: 0.9502 - val_loss: 0.2211 - val_tp: 1175.0000 - val_fp: 322.0000 - val_tn: 4279.0000 - val_fn: 224.0000 - val_accuracy: 0.9090 - val_precision: 0.7849 - val_recall: 0.8399 - val_auc: 0.9597 - val_prc: 0.8879\n",
    "Epoch 10/50\n",
    "750/750 [==============================] - 31s 42ms/step - loss: 0.0849 - tp: 5354.0000 - fp: 1074.0000 - tn: 17298.0000 - fn: 274.0000 - accuracy: 0.9438 - precision: 0.8329 - recall: 0.9513 - auc: 0.9862 - prc: 0.9498 - val_loss: 0.2267 - val_tp: 1251.0000 - val_fp: 428.0000 - val_tn: 4173.0000 - val_fn: 148.0000 - val_accuracy: 0.9040 - val_precision: 0.7451 - val_recall: 0.8942 - val_auc: 0.9634 - val_prc: 0.8893\n",
    "Epoch 11/50\n",
    "750/750 [==============================] - 31s 41ms/step - loss: 0.0823 - tp: 5357.0000 - fp: 1067.0000 - tn: 17305.0000 - fn: 271.0000 - accuracy: 0.9442 - precision: 0.8339 - recall: 0.9518 - auc: 0.9871 - prc: 0.9529 - val_loss: 0.2571 - val_tp: 1226.0000 - val_fp: 432.0000 - val_tn: 4169.0000 - val_fn: 173.0000 - val_accuracy: 0.8992 - val_precision: 0.7394 - val_recall: 0.8763 - val_auc: 0.9587 - val_prc: 0.8802\n",
    "Epoch 12/50\n",
    "750/750 [==============================] - 36s 48ms/step - loss: 0.0791 - tp: 5378.0000 - fp: 995.0000 - tn: 17377.0000 - fn: 250.0000 - accuracy: 0.9481 - precision: 0.8439 - recall: 0.9556 - auc: 0.9881 - prc: 0.9577 - val_loss: 0.2766 - val_tp: 1255.0000 - val_fp: 478.0000 - val_tn: 4123.0000 - val_fn: 144.0000 - val_accuracy: 0.8963 - val_precision: 0.7242 - val_recall: 0.8971 - val_auc: 0.9568 - val_prc: 0.8694\n",
    "Epoch 13/50\n",
    "750/750 [==============================] - 31s 41ms/step - loss: 0.0798 - tp: 5390.0000 - fp: 1036.0000 - tn: 17336.0000 - fn: 238.0000 - accuracy: 0.9469 - precision: 0.8388 - recall: 0.9577 - auc: 0.9877 - prc: 0.9558 - val_loss: 0.2448 - val_tp: 1179.0000 - val_fp: 343.0000 - val_tn: 4258.0000 - val_fn: 220.0000 - val_accuracy: 0.9062 - val_precision: 0.7746 - val_recall: 0.8427 - val_auc: 0.9558 - val_prc: 0.8767\n",
    "Epoch 14/50\n",
    "750/750 [==============================] - 30s 40ms/step - loss: 0.0742 - tp: 5418.0000 - fp: 969.0000 - tn: 17403.0000 - fn: 210.0000 - accuracy: 0.9509 - precision: 0.8483 - recall: 0.9627 - auc: 0.9890 - prc: 0.9599 - val_loss: 0.2540 - val_tp: 1223.0000 - val_fp: 385.0000 - val_tn: 4216.0000 - val_fn: 176.0000 - val_accuracy: 0.9065 - val_precision: 0.7606 - val_recall: 0.8742 - val_auc: 0.9578 - val_prc: 0.8803\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a169b",
   "metadata": {},
   "source": [
    "# Experiment 2: The Class Weights \n",
    "We also adjusted the class weights in the transfer learning notebook. Since this alters the importance of different classes, we want to make sure that this is not the cause of the dropped accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ffa17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.652940408305402, 1: 2.1346235947061336}\n",
      "Epoch 1/5\n",
      "750/750 [==============================] - 72s 95ms/step - loss: 0.1769 - tp: 5366.0000 - fp: 1777.0000 - tn: 16595.0000 - fn: 262.0000 - accuracy: 0.9150 - precision: 0.7512 - recall: 0.9534 - auc: 0.9782 - prc: 0.9249 - val_loss: 0.2092 - val_tp: 1273.0000 - val_fp: 446.0000 - val_tn: 4155.0000 - val_fn: 126.0000 - val_accuracy: 0.9047 - val_precision: 0.7405 - val_recall: 0.9099 - val_auc: 0.9690 - val_prc: 0.8992\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 75s 100ms/step - loss: 0.1663 - tp: 5396.0000 - fp: 1729.0000 - tn: 16643.0000 - fn: 232.0000 - accuracy: 0.9183 - precision: 0.7573 - recall: 0.9588 - auc: 0.9803 - prc: 0.9314 - val_loss: 0.2526 - val_tp: 1328.0000 - val_fp: 577.0000 - val_tn: 4024.0000 - val_fn: 71.0000 - val_accuracy: 0.8920 - val_precision: 0.6971 - val_recall: 0.9492 - val_auc: 0.9711 - val_prc: 0.9038\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 71s 95ms/step - loss: 0.1651 - tp: 5395.0000 - fp: 1651.0000 - tn: 16721.0000 - fn: 233.0000 - accuracy: 0.9215 - precision: 0.7657 - recall: 0.9586 - auc: 0.9808 - prc: 0.9334 - val_loss: 0.2095 - val_tp: 1291.0000 - val_fp: 451.0000 - val_tn: 4150.0000 - val_fn: 108.0000 - val_accuracy: 0.9068 - val_precision: 0.7411 - val_recall: 0.9228 - val_auc: 0.9716 - val_prc: 0.9072\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 76s 101ms/step - loss: 0.1570 - tp: 5384.0000 - fp: 1537.0000 - tn: 16835.0000 - fn: 244.0000 - accuracy: 0.9258 - precision: 0.7779 - recall: 0.9566 - auc: 0.9827 - prc: 0.9392 - val_loss: 0.2356 - val_tp: 1309.0000 - val_fp: 539.0000 - val_tn: 4062.0000 - val_fn: 90.0000 - val_accuracy: 0.8952 - val_precision: 0.7083 - val_recall: 0.9357 - val_auc: 0.9706 - val_prc: 0.9042\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 74s 98ms/step - loss: 0.1547 - tp: 5395.0000 - fp: 1540.0000 - tn: 16832.0000 - fn: 233.0000 - accuracy: 0.9261 - precision: 0.7779 - recall: 0.9586 - auc: 0.9831 - prc: 0.9407 - val_loss: 0.2177 - val_tp: 1267.0000 - val_fp: 406.0000 - val_tn: 4195.0000 - val_fn: 132.0000 - val_accuracy: 0.9103 - val_precision: 0.7573 - val_recall: 0.9056 - val_auc: 0.9688 - val_prc: 0.8993\n"
     ]
    }
   ],
   "source": [
    "xx =compute_class_weight(class_weight='balanced',classes=np.unique(answers.label), y=answers.label)\n",
    "class_weight = dict(zip(np.unique(answers.label), xx))\n",
    "print(class_weight)\n",
    "\n",
    "history = cnn.fit(cb_training, epochs=5,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dee617",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "Accuracy is high, not the class weights. \n",
    "\n",
    "# Experiment 3: Smaller Layers \n",
    "I used a relatively large dropout, and big dense layers, since thats what I saw in the tropical dataset. However, perhaps that is not what we want? \n",
    "Lets compare 2 models:   \n",
    "VGG19 + dense layers from the original CNN  + output using sigmoid \n",
    "and  \n",
    "VGG19 + dense layers from notebook 4, without the Dropout layers + output using sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b20e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 689s 917ms/step - loss: 0.3793 - tp: 6242.0000 - fp: 4394.0000 - tn: 18579.0000 - fn: 785.0000 - accuracy: 0.8274 - precision: 0.5869 - recall: 0.8883 - auc: 0.9231 - prc: 0.7751 - val_loss: 0.3350 - val_tp: 1283.0000 - val_fp: 947.0000 - val_tn: 3654.0000 - val_fn: 116.0000 - val_accuracy: 0.8228 - val_precision: 0.5753 - val_recall: 0.9171 - val_auc: 0.9343 - val_prc: 0.8022\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 61589s 82s/step - loss: 0.3066 - tp: 5233.0000 - fp: 3713.0000 - tn: 14659.0000 - fn: 395.0000 - accuracy: 0.8288 - precision: 0.5850 - recall: 0.9298 - auc: 0.9393 - prc: 0.8106 - val_loss: 0.4066 - val_tp: 1340.0000 - val_fp: 1184.0000 - val_tn: 3417.0000 - val_fn: 59.0000 - val_accuracy: 0.7928 - val_precision: 0.5309 - val_recall: 0.9578 - val_auc: 0.9382 - val_prc: 0.8152\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 621s 829ms/step - loss: 0.2833 - tp: 5315.0000 - fp: 3609.0000 - tn: 14763.0000 - fn: 313.0000 - accuracy: 0.8366 - precision: 0.5956 - recall: 0.9444 - auc: 0.9475 - prc: 0.8340 - val_loss: 0.3044 - val_tp: 1202.0000 - val_fp: 753.0000 - val_tn: 3848.0000 - val_fn: 197.0000 - val_accuracy: 0.8417 - val_precision: 0.6148 - val_recall: 0.8592 - val_auc: 0.9361 - val_prc: 0.8171\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 639s 853ms/step - loss: 0.2670 - tp: 5353.0000 - fp: 3529.0000 - tn: 14843.0000 - fn: 275.0000 - accuracy: 0.8415 - precision: 0.6027 - recall: 0.9511 - auc: 0.9529 - prc: 0.8498 - val_loss: 0.3264 - val_tp: 1306.0000 - val_fp: 1055.0000 - val_tn: 3546.0000 - val_fn: 93.0000 - val_accuracy: 0.8087 - val_precision: 0.5532 - val_recall: 0.9335 - val_auc: 0.9348 - val_prc: 0.8168\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 638s 851ms/step - loss: 0.2563 - tp: 5165.0000 - fp: 2669.0000 - tn: 15703.0000 - fn: 463.0000 - accuracy: 0.8695 - precision: 0.6593 - recall: 0.9177 - auc: 0.9560 - prc: 0.8568 - val_loss: 0.2933 - val_tp: 1162.0000 - val_fp: 575.0000 - val_tn: 4026.0000 - val_fn: 237.0000 - val_accuracy: 0.8647 - val_precision: 0.6690 - val_recall: 0.8306 - val_auc: 0.9387 - val_prc: 0.8277\n"
     ]
    }
   ],
   "source": [
    "# Dense layers original CNN \n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(97,97,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model =  Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(cb_training, epochs=5,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155f23e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "750/750 [==============================] - 635s 843ms/step - loss: 0.4540 - tp: 5945.0000 - fp: 4210.0000 - tn: 18763.0000 - fn: 1082.0000 - accuracy: 0.8236 - precision: 0.5854 - recall: 0.8460 - auc: 0.9041 - prc: 0.7013 - val_loss: 0.3045 - val_tp: 1114.0000 - val_fp: 561.0000 - val_tn: 4040.0000 - val_fn: 285.0000 - val_accuracy: 0.8590 - val_precision: 0.6651 - val_recall: 0.7963 - val_auc: 0.9286 - val_prc: 0.7893\n",
      "Epoch 2/2\n",
      "750/750 [==============================] - 652s 869ms/step - loss: 0.3239 - tp: 5036.0000 - fp: 3154.0000 - tn: 15218.0000 - fn: 592.0000 - accuracy: 0.8439 - precision: 0.6149 - recall: 0.8948 - auc: 0.9333 - prc: 0.7966 - val_loss: 0.3481 - val_tp: 1248.0000 - val_fp: 794.0000 - val_tn: 3807.0000 - val_fn: 151.0000 - val_accuracy: 0.8425 - val_precision: 0.6112 - val_recall: 0.8921 - val_auc: 0.9363 - val_prc: 0.8081\n"
     ]
    }
   ],
   "source": [
    "# Dense Layers from notebook 4 \n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(97,97,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model =  Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(cb_training, epochs=2,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96950ba7",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "The accuracy and other metrics are similar between the two model instances, so it looks like the cause of the drop is not due to the difference in the dense layers.  \n",
    "Also the metrics are so similar, I would have trouble choosing the model based on so few epochs. \n",
    "\n",
    "# Experiment 4 Output \n",
    "Try softmax as the final layer, using the same parameters as the previous experiment besides that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5029bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "750/750 [==============================] - 662s 877ms/step - loss: 0.4746 - tp: 6876.0000 - fp: 19166.0000 - tn: 3807.0000 - fn: 151.0000 - accuracy: 0.3561 - precision: 0.2640 - recall: 0.9785 - auc: 0.5284 - prc: 0.2429 - val_loss: 0.3493 - val_tp: 1399.0000 - val_fp: 4601.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.2332 - val_precision: 0.2332 - val_recall: 1.0000 - val_auc: 0.5000 - val_prc: 0.2332\n",
      "Epoch 2/2\n",
      "750/750 [==============================] - 656s 875ms/step - loss: 0.3312 - tp: 5628.0000 - fp: 18372.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.2345 - precision: 0.2345 - recall: 1.0000 - auc: 0.5000 - prc: 0.2345 - val_loss: 0.3234 - val_tp: 1399.0000 - val_fp: 4601.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.2332 - val_precision: 0.2332 - val_recall: 1.0000 - val_auc: 0.5000 - val_prc: 0.2332\n"
     ]
    }
   ],
   "source": [
    "# Softmax \n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(97,97,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(1, activation='softmax')(x)\n",
    "\n",
    "model =  Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(cb_training, epochs=2,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871fdd6",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "There it is! Theres the drop in accuracy! \n",
    "Softmax causes a massive drop in accuracy, as well as weirdness in the validation true positive and negative counts.  \n",
    "## I've made errors in Notebook 4 \n",
    "Softmax is multiclass - Im operating on a binary problem. This wouldnt be a problem, since 2 classes is still  multiple classes. My output, however, is not in the one-hot-encoding format, (it is in 1 column), so trying to give dense 2 classes was giving an error, and I, in a very silly way, classified stuff as 1 class.  \n",
    "Since we're in a binary problem, Sigmoid is good to use. \n",
    "https://medium.com/arteos-ai/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322 \n",
    "\n",
    "# Experiment 5 Dropout \n",
    "Since the original CNN overfit, I wanted to add a dropout layer. The tropical classification paper I've been referencing also has a dropout (rather large, .5). I set mine slightly lower - at .3. Lets give it a try. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e667d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "750/750 [==============================] - 681s 905ms/step - loss: 0.5821 - tp: 6362.0000 - fp: 11039.0000 - tn: 11934.0000 - fn: 665.0000 - accuracy: 0.6099 - precision: 0.3656 - recall: 0.9054 - auc: 0.7215 - prc: 0.3403 - val_loss: 0.4127 - val_tp: 1352.0000 - val_fp: 1588.0000 - val_tn: 3013.0000 - val_fn: 47.0000 - val_accuracy: 0.7275 - val_precision: 0.4599 - val_recall: 0.9664 - val_auc: 0.9086 - val_prc: 0.7524\n",
      "Epoch 2/2\n",
      "750/750 [==============================] - 657s 876ms/step - loss: 0.4074 - tp: 5331.0000 - fp: 6298.0000 - tn: 12074.0000 - fn: 297.0000 - accuracy: 0.7252 - precision: 0.4584 - recall: 0.9472 - auc: 0.8925 - prc: 0.7166 - val_loss: 0.2994 - val_tp: 1220.0000 - val_fp: 793.0000 - val_tn: 3808.0000 - val_fn: 179.0000 - val_accuracy: 0.8380 - val_precision: 0.6061 - val_recall: 0.8721 - val_auc: 0.9317 - val_prc: 0.7943\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(97,97,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(.3)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(.3)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model =  Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(cb_training, epochs=2,validation_data=cb_validation, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae23562",
   "metadata": {},
   "source": [
    "# Train Again\n",
    "\n",
    "https://upcommons.upc.edu/bitstream/handle/2117/175744/131673.pdf?sequence=1&isAllowed=y\n",
    "\n",
    "2 dense layers seems to be the standard used in both the overview above, as well as the Tropical Classification. \n",
    "Trying a bit of a larger layer, but with dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3963b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "750/750 [==============================] - 685s 910ms/step - loss: 1.1276 - tp: 5235.0000 - fp: 6241.0000 - tn: 13725.0000 - fn: 911.0000 - accuracy: 0.7261 - precision: 0.4562 - recall: 0.8518 - auc: 0.8224 - prc: 0.5171 - val_loss: 0.3629 - val_tp: 1140.0000 - val_fp: 892.0000 - val_tn: 3709.0000 - val_fn: 259.0000 - val_accuracy: 0.8082 - val_precision: 0.5610 - val_recall: 0.8149 - val_auc: 0.8950 - val_prc: 0.7160\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 727s 970ms/step - loss: 0.4743 - tp: 5267.0000 - fp: 6992.0000 - tn: 11380.0000 - fn: 361.0000 - accuracy: 0.6936 - precision: 0.4296 - recall: 0.9359 - auc: 0.8614 - prc: 0.6456 - val_loss: 0.3824 - val_tp: 1326.0000 - val_fp: 1293.0000 - val_tn: 3308.0000 - val_fn: 73.0000 - val_accuracy: 0.7723 - val_precision: 0.5063 - val_recall: 0.9478 - val_auc: 0.9182 - val_prc: 0.7654\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 726s 968ms/step - loss: 0.4424 - tp: 5294.0000 - fp: 6292.0000 - tn: 12080.0000 - fn: 334.0000 - accuracy: 0.7239 - precision: 0.4569 - recall: 0.9407 - auc: 0.8798 - prc: 0.6845 - val_loss: 0.3367 - val_tp: 1252.0000 - val_fp: 996.0000 - val_tn: 3605.0000 - val_fn: 147.0000 - val_accuracy: 0.8095 - val_precision: 0.5569 - val_recall: 0.8949 - val_auc: 0.9165 - val_prc: 0.7545\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 802s 1s/step - loss: 0.4261 - tp: 5280.0000 - fp: 5887.0000 - tn: 12485.0000 - fn: 348.0000 - accuracy: 0.7402 - precision: 0.4728 - recall: 0.9382 - auc: 0.8857 - prc: 0.7027 - val_loss: 0.4241 - val_tp: 1289.0000 - val_fp: 1201.0000 - val_tn: 3400.0000 - val_fn: 110.0000 - val_accuracy: 0.7815 - val_precision: 0.5177 - val_recall: 0.9214 - val_auc: 0.8944 - val_prc: 0.6811\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 737s 983ms/step - loss: 0.3993 - tp: 5219.0000 - fp: 5277.0000 - tn: 13095.0000 - fn: 409.0000 - accuracy: 0.7631 - precision: 0.4972 - recall: 0.9273 - auc: 0.8895 - prc: 0.7145 - val_loss: 0.3514 - val_tp: 1205.0000 - val_fp: 878.0000 - val_tn: 3723.0000 - val_fn: 194.0000 - val_accuracy: 0.8213 - val_precision: 0.5785 - val_recall: 0.8613 - val_auc: 0.9042 - val_prc: 0.7441\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 747s 996ms/step - loss: 0.3972 - tp: 5200.0000 - fp: 5157.0000 - tn: 13215.0000 - fn: 428.0000 - accuracy: 0.7673 - precision: 0.5021 - recall: 0.9240 - auc: 0.8887 - prc: 0.7183 - val_loss: 0.4014 - val_tp: 1260.0000 - val_fp: 936.0000 - val_tn: 3665.0000 - val_fn: 139.0000 - val_accuracy: 0.8208 - val_precision: 0.5738 - val_recall: 0.9006 - val_auc: 0.9051 - val_prc: 0.7386\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 745s 994ms/step - loss: 0.4035 - tp: 5214.0000 - fp: 5246.0000 - tn: 13126.0000 - fn: 414.0000 - accuracy: 0.7642 - precision: 0.4985 - recall: 0.9264 - auc: 0.8800 - prc: 0.6886 - val_loss: 0.3694 - val_tp: 1286.0000 - val_fp: 992.0000 - val_tn: 3609.0000 - val_fn: 113.0000 - val_accuracy: 0.8158 - val_precision: 0.5645 - val_recall: 0.9192 - val_auc: 0.8995 - val_prc: 0.7076\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 737s 983ms/step - loss: 0.4106 - tp: 5170.0000 - fp: 4890.0000 - tn: 13482.0000 - fn: 458.0000 - accuracy: 0.7772 - precision: 0.5139 - recall: 0.9186 - auc: 0.8718 - prc: 0.6438 - val_loss: 0.3756 - val_tp: 1233.0000 - val_fp: 825.0000 - val_tn: 3776.0000 - val_fn: 166.0000 - val_accuracy: 0.8348 - val_precision: 0.5991 - val_recall: 0.8813 - val_auc: 0.8994 - val_prc: 0.6585\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 871s 1s/step - loss: 0.4186 - tp: 5115.0000 - fp: 4667.0000 - tn: 13705.0000 - fn: 513.0000 - accuracy: 0.7842 - precision: 0.5229 - recall: 0.9088 - auc: 0.8572 - prc: 0.5415 - val_loss: 0.3769 - val_tp: 1276.0000 - val_fp: 866.0000 - val_tn: 3735.0000 - val_fn: 123.0000 - val_accuracy: 0.8352 - val_precision: 0.5957 - val_recall: 0.9121 - val_auc: 0.9000 - val_prc: 0.6207\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 1151s 2s/step - loss: 0.4164 - tp: 5137.0000 - fp: 4739.0000 - tn: 13633.0000 - fn: 491.0000 - accuracy: 0.7821 - precision: 0.5201 - recall: 0.9128 - auc: 0.8576 - prc: 0.5316 - val_loss: 0.3716 - val_tp: 1228.0000 - val_fp: 783.0000 - val_tn: 3818.0000 - val_fn: 171.0000 - val_accuracy: 0.8410 - val_precision: 0.6106 - val_recall: 0.8778 - val_auc: 0.8998 - val_prc: 0.6208\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 1148s 2s/step - loss: 0.4007 - tp: 5073.0000 - fp: 4224.0000 - tn: 14148.0000 - fn: 555.0000 - accuracy: 0.8009 - precision: 0.5457 - recall: 0.9014 - auc: 0.8703 - prc: 0.5569 - val_loss: 0.3532 - val_tp: 1272.0000 - val_fp: 833.0000 - val_tn: 3768.0000 - val_fn: 127.0000 - val_accuracy: 0.8400 - val_precision: 0.6043 - val_recall: 0.9092 - val_auc: 0.9013 - val_prc: 0.6219\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 913s 1s/step - loss: 0.3936 - tp: 5115.0000 - fp: 4150.0000 - tn: 14222.0000 - fn: 513.0000 - accuracy: 0.8057 - precision: 0.5521 - recall: 0.9088 - auc: 0.8756 - prc: 0.5673 - val_loss: 0.4537 - val_tp: 1272.0000 - val_fp: 825.0000 - val_tn: 3776.0000 - val_fn: 127.0000 - val_accuracy: 0.8413 - val_precision: 0.6066 - val_recall: 0.9092 - val_auc: 0.9172 - val_prc: 0.6794\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 685s 913ms/step - loss: 0.3830 - tp: 5109.0000 - fp: 4078.0000 - tn: 14294.0000 - fn: 519.0000 - accuracy: 0.8085 - precision: 0.5561 - recall: 0.9078 - auc: 0.8788 - prc: 0.5727 - val_loss: 0.4042 - val_tp: 1110.0000 - val_fp: 492.0000 - val_tn: 4109.0000 - val_fn: 289.0000 - val_accuracy: 0.8698 - val_precision: 0.6929 - val_recall: 0.7934 - val_auc: 0.9306 - val_prc: 0.7461\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 681s 908ms/step - loss: 0.3826 - tp: 5064.0000 - fp: 3904.0000 - tn: 14468.0000 - fn: 564.0000 - accuracy: 0.8138 - precision: 0.5647 - recall: 0.8998 - auc: 0.8830 - prc: 0.5788 - val_loss: 0.3505 - val_tp: 1322.0000 - val_fp: 908.0000 - val_tn: 3693.0000 - val_fn: 77.0000 - val_accuracy: 0.8358 - val_precision: 0.5928 - val_recall: 0.9450 - val_auc: 0.9242 - val_prc: 0.7022\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 685s 913ms/step - loss: 0.3760 - tp: 5223.0000 - fp: 4268.0000 - tn: 14104.0000 - fn: 405.0000 - accuracy: 0.8053 - precision: 0.5503 - recall: 0.9280 - auc: 0.8817 - prc: 0.5775 - val_loss: 0.3696 - val_tp: 1315.0000 - val_fp: 922.0000 - val_tn: 3679.0000 - val_fn: 84.0000 - val_accuracy: 0.8323 - val_precision: 0.5878 - val_recall: 0.9400 - val_auc: 0.9110 - val_prc: 0.6480\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 682s 910ms/step - loss: 0.3856 - tp: 5205.0000 - fp: 4372.0000 - tn: 14000.0000 - fn: 423.0000 - accuracy: 0.8002 - precision: 0.5435 - recall: 0.9248 - auc: 0.8748 - prc: 0.5601 - val_loss: 0.3480 - val_tp: 1304.0000 - val_fp: 890.0000 - val_tn: 3711.0000 - val_fn: 95.0000 - val_accuracy: 0.8358 - val_precision: 0.5943 - val_recall: 0.9321 - val_auc: 0.9147 - val_prc: 0.6603\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 683s 911ms/step - loss: 0.3790 - tp: 5162.0000 - fp: 4135.0000 - tn: 14237.0000 - fn: 466.0000 - accuracy: 0.8083 - precision: 0.5552 - recall: 0.9172 - auc: 0.8816 - prc: 0.5857 - val_loss: 0.3725 - val_tp: 1290.0000 - val_fp: 820.0000 - val_tn: 3781.0000 - val_fn: 109.0000 - val_accuracy: 0.8452 - val_precision: 0.6114 - val_recall: 0.9221 - val_auc: 0.9301 - val_prc: 0.7262\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 686s 914ms/step - loss: 0.3734 - tp: 5138.0000 - fp: 4069.0000 - tn: 14303.0000 - fn: 490.0000 - accuracy: 0.8100 - precision: 0.5581 - recall: 0.9129 - auc: 0.8875 - prc: 0.5995 - val_loss: 0.3457 - val_tp: 1305.0000 - val_fp: 855.0000 - val_tn: 3746.0000 - val_fn: 94.0000 - val_accuracy: 0.8418 - val_precision: 0.6042 - val_recall: 0.9328 - val_auc: 0.9226 - val_prc: 0.6895\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 684s 912ms/step - loss: 0.3733 - tp: 5211.0000 - fp: 4149.0000 - tn: 14223.0000 - fn: 417.0000 - accuracy: 0.8098 - precision: 0.5567 - recall: 0.9259 - auc: 0.8856 - prc: 0.5877 - val_loss: 0.3897 - val_tp: 1316.0000 - val_fp: 914.0000 - val_tn: 3687.0000 - val_fn: 83.0000 - val_accuracy: 0.8338 - val_precision: 0.5901 - val_recall: 0.9407 - val_auc: 0.9080 - val_prc: 0.6392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "750/750 [==============================] - 688s 917ms/step - loss: 0.3669 - tp: 5298.0000 - fp: 4315.0000 - tn: 14057.0000 - fn: 330.0000 - accuracy: 0.8065 - precision: 0.5511 - recall: 0.9414 - auc: 0.8821 - prc: 0.5769 - val_loss: 0.3413 - val_tp: 1291.0000 - val_fp: 835.0000 - val_tn: 3766.0000 - val_fn: 108.0000 - val_accuracy: 0.8428 - val_precision: 0.6072 - val_recall: 0.9228 - val_auc: 0.9096 - val_prc: 0.6437\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 687s 916ms/step - loss: 0.3726 - tp: 5284.0000 - fp: 4387.0000 - tn: 13985.0000 - fn: 344.0000 - accuracy: 0.8029 - precision: 0.5464 - recall: 0.9389 - auc: 0.8799 - prc: 0.5728 - val_loss: 0.3897 - val_tp: 1326.0000 - val_fp: 940.0000 - val_tn: 3661.0000 - val_fn: 73.0000 - val_accuracy: 0.8312 - val_precision: 0.5852 - val_recall: 0.9478 - val_auc: 0.9065 - val_prc: 0.6332\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 685s 914ms/step - loss: 0.3736 - tp: 5257.0000 - fp: 4317.0000 - tn: 14055.0000 - fn: 371.0000 - accuracy: 0.8047 - precision: 0.5491 - recall: 0.9341 - auc: 0.8820 - prc: 0.5800 - val_loss: 0.3494 - val_tp: 1306.0000 - val_fp: 863.0000 - val_tn: 3738.0000 - val_fn: 93.0000 - val_accuracy: 0.8407 - val_precision: 0.6021 - val_recall: 0.9335 - val_auc: 0.9271 - val_prc: 0.7094\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 685s 913ms/step - loss: 0.3688 - tp: 5216.0000 - fp: 4162.0000 - tn: 14210.0000 - fn: 412.0000 - accuracy: 0.8094 - precision: 0.5562 - recall: 0.9268 - auc: 0.8878 - prc: 0.5893 - val_loss: 0.3259 - val_tp: 1320.0000 - val_fp: 873.0000 - val_tn: 3728.0000 - val_fn: 79.0000 - val_accuracy: 0.8413 - val_precision: 0.6019 - val_recall: 0.9435 - val_auc: 0.9396 - val_prc: 0.7739\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 687s 916ms/step - loss: 0.3598 - tp: 5251.0000 - fp: 4092.0000 - tn: 14280.0000 - fn: 377.0000 - accuracy: 0.8138 - precision: 0.5620 - recall: 0.9330 - auc: 0.8916 - prc: 0.6021 - val_loss: 0.3714 - val_tp: 1335.0000 - val_fp: 1019.0000 - val_tn: 3582.0000 - val_fn: 64.0000 - val_accuracy: 0.8195 - val_precision: 0.5671 - val_recall: 0.9543 - val_auc: 0.9217 - val_prc: 0.6850\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 777s 1s/step - loss: 0.3612 - tp: 5226.0000 - fp: 4057.0000 - tn: 14315.0000 - fn: 402.0000 - accuracy: 0.8142 - precision: 0.5630 - recall: 0.9286 - auc: 0.8911 - prc: 0.6036 - val_loss: 0.4062 - val_tp: 1358.0000 - val_fp: 1138.0000 - val_tn: 3463.0000 - val_fn: 41.0000 - val_accuracy: 0.8035 - val_precision: 0.5441 - val_recall: 0.9707 - val_auc: 0.9181 - val_prc: 0.6705\n",
      "Epoch 26/30\n",
      "169/750 [=====>........................] - ETA: 13:32 - loss: 0.3645 - tp: 1216.0000 - fp: 986.0000 - tn: 3140.0000 - fn: 66.0000 - accuracy: 0.8055 - precision: 0.5522 - recall: 0.9485 - auc: 0.8820 - prc: 0.5743"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(97,97,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(500, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model =  Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(cb_training, epochs=30,validation_data=cb_validation, class_weight = class_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
